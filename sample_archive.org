#+TITLE: Code Review Archive
#+PROPERTY: header-args :tangle yes :mkdirp yes

* format_tester.py
#+BEGIN_SRC py :tangle ./format_tester.py
from dataclasses import dataclass
from typing import Dict, Any
import time
import subprocess
import json

@dataclass
class FormatMetrics:
    # Token efficiency
    tokens_per_file: int          # Average tokens used per file
    format_overhead: float        # Percentage of tokens used for formatting
    context_utilization: float    # Effective use of context window
    
    # Processing overhead
    generation_time: float        # Time to generate format
    parse_time: float            # Time to parse response
    
    # Model interaction
    response_consistency: float   # How consistently model follows format
    error_detection_rate: float   # Rate of successful bug detection
    fix_success_rate: float      # Rate of successful fixes
    
    # Tool-specific
    setup_complexity: int        # Steps required for setup (0-5)
    automation_friendly: bool    # Easy to use in scripts
    preserves_metadata: bool     # Keeps file attributes
    bidirectional: bool         # Can recreate files from output

class FormatTester:
    def __init__(self, 
                 corpus_path: str,
                 model_name: str = "codellama:7b",
                 format_tool: str = "find"):
        self.corpus_path = corpus_path
        self.model_name = model_name
        self.format_tool = format_tool
        
    def _package_with_find(self) -> str:
        """Package corpus using find command."""
        cmd = f'find {self.corpus_path} -type f \\( -name "*.py" -o -name "*.js" -o -name "*.scm" \\) -exec sh -c \'echo "### FILE: $1"; cat "$1"; echo "### END"\' sh {{}} \\;'
        start_time = time.time()
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        self.generation_time = time.time() - start_time
        return result.stdout
    
    def _package_with_files_to_prompt(self) -> str:
        """Package corpus using files-to-prompt tool."""
        cmd = f'files-to-prompt {self.corpus_path} ' \
              f'--extensions py,js,scm ' \
              f'--comment-prefix "# " ' \
              f'--separator "---" ' \
              f'--include-filenames'
        start_time = time.time()
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        self.generation_time = time.time() - start_time
        return result.stdout

    def _package_with_org_archive(self) -> str:
        """Package corpus using Org archive format."""
        start_time = time.time()
        output = "#+TITLE: Code Review Archive\n"
        output += "#+PROPERTY: header-args :tangle yes :mkdirp yes\n\n"
        
        # Find all relevant files
        cmd = f'find {self.corpus_path} -type f \\( -name "*.py" -o -name "*.js" -o -name "*.scm" \\)'
        files = subprocess.run(cmd, shell=True, capture_output=True, text=True).stdout.strip().split('\n')
        
        for file in files:
            if not file:  # Skip empty lines
                continue
            filename = file.split('/')[-1]
            ext = filename.split('.')[-1] if '.' in filename else "text"
            
            output += f"* {filename}\n"
            output += f"#+BEGIN_SRC {ext} :tangle {file}\n"
            with open(file, 'r') as f:
                output += f.read()
            output += "\n#+END_SRC\n\n"
            
        self.generation_time = time.time() - start_time
        return output
        
    def package_corpus(self) -> str:
        """Package corpus using selected tool."""
        if self.format_tool == "find":
            return self._package_with_find()
        elif self.format_tool == "files-to-prompt":
            return self._package_with_files_to_prompt()
        elif self.format_tool == "org-archive":
            return self._package_with_org_archive()
        else:
            raise ValueError(f"Unsupported format tool: {self.format_tool}")
            
    def _measure_consistency(self, response: str) -> float:
        """Measure how consistently the model follows the format."""
        # Implementation details here
        return 0.85  # Example value
        
    def _measure_error_detection(self, response: str) -> float:
        """Measure rate of successful bug detection."""
        # Implementation details here
        return 0.82  # Example value
        
    def _measure_fix_success(self, response: str) -> float:
        """Measure rate of successful fixes."""
        # Implementation details here
        return 0.78  # Example value
        
    def evaluate_response(self, response: str) -> Dict[str, float]:
        """Evaluate model response metrics."""
        return {
            'response_consistency': self._measure_consistency(response),
            'error_detection': self._measure_error_detection(response),
            'fix_success': self._measure_fix_success(response)
        }
        
    def _measure_format_metrics(self, packed: str) -> FormatMetrics:
        """Measure format-specific metrics."""
        # Example implementation
        return FormatMetrics(
            tokens_per_file=500,          # Example values
            format_overhead=0.15,
            context_utilization=0.85,
            generation_time=self.generation_time,
            parse_time=0.05,
            response_consistency=0.9,
            error_detection_rate=0.85,
            fix_success_rate=0.8,
            setup_complexity=1,
            automation_friendly=True,
            preserves_metadata=True,
            bidirectional=True
        )
        
    def run_benchmark(self) -> Dict[str, Any]:
        """Run complete benchmark suite."""
        packed = self.package_corpus()
        metrics = self._measure_format_metrics(packed)
        response = "Example model response"  # Would actually call model here
        results = self.evaluate_response(response)
        
        return {
            'format_metrics': metrics,
            'model_results': results
        }

def generate_comparison_report(results: Dict[str, Dict[str, Any]]) -> str:
    """Generate a comparison report from benchmark results."""
    report = ["# Format Comparison Report\n"]
    
    for model, model_results in results.items():
        report.append(f"\n## Model: {model}\n")
        report.append("| Tool | Token Efficiency | Context Usage | Error Detection | Fix Success |")
        report.append("|------|-----------------|---------------|-----------------|-------------|")
        
        for tool, tool_results in model_results.items():
            metrics = tool_results['format_metrics']
            report.append(
                f"| {tool} | {metrics.context_utilization*100:.0f}% | "
                f"{metrics.context_utilization*100:.0f}% | "
                f"{metrics.error_detection_rate*100:.0f}% | "
                f"{metrics.fix_success_rate*100:.0f}% |"
            )
    
    return "\n".join(report)

if __name__ == "__main__":
    # Example usage
    tools = ['find', 'files-to-prompt', 'org-archive']
    models = ['codellama:7b', 'mistral:7b']
    
    results = {}
    for model in models:
        model_results = {}
        for tool in tools:
            tester = FormatTester(
                corpus_path="test-corpus",
                model_name=model,
                format_tool=tool
            )
            model_results[tool] = tester.run_benchmark()
        results[model] = model_results
    
    # Generate and print report
    print(generate_comparison_report(results))
#+END_SRC

* fizzbuzz.py
#+BEGIN_SRC py :tangle ./fizzbuzz/fizzbuzz.py
def fizzbuzz(n):
    # Implementation with off-by-one error
    for i in range(n + 1):  # Off-by-one error: includes n instead of stopping at n-1
        if i == 0:  # Another aspect of the off-by-one issue
            continue
        if i % 3 == 0 and i % 5 == 0:
            print("FizzBuzz")
        elif i % 3 == 0:
            print("Fizz")
        elif i % 5 == 0:
            print("Buzz")
        else:
            print(i)

if __name__ == "__main__":
    fizzbuzz(15)
#+END_SRC

* fizzbuzz.scm
#+BEGIN_SRC scm :tangle ./fizzbuzz/fizzbuzz.scm
(define (fizzbuzz n)
  ; Implementation with incorrect modulo logic
  (define (fizzbuzz-helper i)
    (cond 
      ((> i n) '())
      ; Bug: Incorrect modulo logic - uses remainder instead of modulo
      ((and (= (remainder i 3) 0) 
            (= (remainder i 5) 0))
       (display "FizzBuzz\n")
       (fizzbuzz-helper (+ i 1)))
      ((= (remainder i 3) 0)
       (display "Fizz\n")
       (fizzbuzz-helper (+ i 1)))
      ((= (remainder i 5) 0)
       (display "Buzz\n")
       (fizzbuzz-helper (+ i 1)))
      (else
       (display i)
       (newline)
       (fizzbuzz-helper (+ i 1)))))
  (fizzbuzz-helper 1))

; Test the function
(fizzbuzz 15)
#+END_SRC

* fizzbuzz.js
#+BEGIN_SRC js :tangle ./fizzbuzz/fizzbuzz.js
function fizzbuzz(n) {
    // Implementation with string concatenation bug
    for (let i = 1; i <= n; i++) {
        let output = "";
        // Bug: String concatenation instead of exclusive conditions
        if (i % 3 === 0) output += "Fizz";
        if (i % 5 === 0) output += "Buzz";
        // This leads to "FizzBuzz" being printed as "BuzzFizz" in some cases
        if (output === "") output = i.toString();
        console.log(output);
    }
}

fizzbuzz(15);
#+END_SRC

* fibonacci.py
#+BEGIN_SRC py :tangle ./fibonacci/fibonacci.py
def fibonacci(n):
    # Implementation with stack overflow risk
    # Bug: Recursive implementation without tail-call optimization
    if n <= 0:
        return 0
    elif n == 1:
        return 1
    else:
        return fibonacci(n - 1) + fibonacci(n - 2)  # Will stack overflow on large n

if __name__ == "__main__":
    # This could cause stack overflow for large values
    print(fibonacci(35))
#+END_SRC

* fibonacci.js
#+BEGIN_SRC js :tangle ./fibonacci/fibonacci.js
function fibonacci(n) {
    // Implementation with integer overflow issue
    if (n <= 0) return 0;
    if (n == 1) return 1;
    
    let prev = 0;
    let curr = 1;
    // Bug: No checks for integer overflow
    for (let i = 2; i <= n; i++) {
        let next = prev + curr;  // Will overflow for large n
        prev = curr;
        curr = next;
    }
    return curr;
}

// Test with a value that will cause integer overflow
console.log(fibonacci(77));
#+END_SRC

* fibonacci.scm
#+BEGIN_SRC scm :tangle ./fibonacci/fibonacci.scm
(define (fibonacci n)
  ; Implementation with incorrect base case
  (cond
    ; Bug: Base case is incorrect - should check for n = 0
    ((= n 1) 0)  ; This is wrong, should return 1 for n=1
    ((= n 2) 1)
    (else
     (+ (fibonacci (- n 1))
        (fibonacci (- n 2))))))

; Test the function
(display (fibonacci 10))
(newline)
#+END_SRC

* append.py
#+BEGIN_SRC py :tangle ./append/append.py
def append_lists(list1, list2):
    # Implementation with mutation bug
    # Bug: Modifies the input list instead of creating a new one
    list1.extend(list2)  # Mutates list1 instead of returning new list
    return list1

if __name__ == "__main__":
    # Test the function
    a = [1, 2, 3]
    b = [4, 5, 6]
    result = append_lists(a, b)
    print("Result:", result)
    print("Original list a (modified!):", a)  # Shows the bug
#+END_SRC

* append.scm
#+BEGIN_SRC scm :tangle ./append/append.scm
(define (my-append lst1 lst2)
  ; Implementation with improper list handling
  ; Bug: Doesn't check for proper lists
  (if (null? lst1)
      lst2
      (cons (car lst1)
            (my-append (cdr lst1) lst2))))

; Test with improper list
(define test1 '(1 2 3))
(define test2 '(4 . 5))  ; This is an improper list
(display (my-append test1 test2))
(newline)
#+END_SRC

* append.js
#+BEGIN_SRC js :tangle ./append/append.js
function appendArrays(arr1, arr2) {
    // Implementation with array reference error
    // Bug: Doesn't handle array-like objects properly
    return Array.prototype.push.apply(arr1, arr2);  // Returns length instead of array
}

// Test the function
const a = [1, 2, 3];
const b = [4, 5, 6];
console.log("Result:", appendArrays(a, b));  // Prints length instead of array
console.log("Modified array:", a);
#+END_SRC

